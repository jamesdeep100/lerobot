#!/usr/bin/env python3
"""
é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒ Diffusion Policy å’Œ ACT

ç”¨æ³•:
    python eval_model.py --model_path <æ¨¡å‹è·¯å¾„> [é€‰é¡¹]

ç¤ºä¾‹:
    python eval_model.py --model_path outputs/diffusion_exp/exp1_100k --policy_type diffusion
    python eval_model.py --model_path outputs/act_exp/exp1 --policy_type act --n_episodes 20
"""

import argparse
import sys
import time
import json
from pathlib import Path

import torch
import numpy as np
import gymnasium as gym

# ç¡®ä¿è¾“å‡ºå®æ—¶åˆ·æ–°
def log(msg):
    print(msg, flush=True)


def load_policy(model_path: str, policy_type: str):
    """åŠ è½½ç­–ç•¥æ¨¡å‹"""
    log(f"[{time.strftime('%H:%M:%S')}] ğŸ“¦ åŠ è½½ {policy_type} æ¨¡å‹: {model_path}")
    
    if policy_type == "diffusion":
        from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy
        policy = DiffusionPolicy.from_pretrained(model_path)
    elif policy_type == "act":
        from lerobot.policies.act.modeling_act import ACTPolicy
        policy = ACTPolicy.from_pretrained(model_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {policy_type}")
    
    policy.eval()
    policy.cuda()
    
    params = sum(p.numel() for p in policy.parameters())
    log(f"[{time.strftime('%H:%M:%S')}] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    log(f"   å‚æ•°é‡: {params:,} ({params/1e9:.2f}B)")
    
    # æ‰“å°å…³é”®é…ç½®
    if hasattr(policy.config, 'horizon'):
        log(f"   horizon: {policy.config.horizon}")
    if hasattr(policy.config, 'n_action_steps'):
        log(f"   n_action_steps: {policy.config.n_action_steps}")
    if hasattr(policy.config, 'down_dims'):
        log(f"   down_dims: {policy.config.down_dims}")
    if hasattr(policy.config, 'dim_model'):
        log(f"   dim_model: {policy.config.dim_model}")
    if hasattr(policy.config, 'n_decoder_layers'):
        log(f"   n_decoder_layers: {policy.config.n_decoder_layers}")
    
    return policy


def evaluate(policy, n_episodes: int = 50, verbose: bool = True):
    """è¯„ä¼°æ¨¡å‹"""
    import gym_pusht  # ç¡®ä¿ç¯å¢ƒå·²æ³¨å†Œ
    
    log(f"\n[{time.strftime('%H:%M:%S')}] ğŸ® åˆ›å»º PushT ç¯å¢ƒ...")
    env = gym.make("gym_pusht/PushT-v0", obs_type="pixels_agent_pos", render_mode="rgb_array")
    
    successes = []
    sum_rewards = []
    max_rewards = []
    episode_times = []
    
    log(f"[{time.strftime('%H:%M:%S')}] ğŸš€ å¼€å§‹è¯„ä¼° ({n_episodes} episodes)...")
    log("=" * 60)
    
    total_start = time.time()
    
    for ep in range(n_episodes):
        ep_start = time.time()
        obs, info = env.reset()
        policy.reset()
        done = False
        episode_reward = 0
        max_reward = 0
        step = 0
        
        while not done:
            # å‡†å¤‡è¾“å…¥
            img = torch.from_numpy(obs["pixels"]).float().permute(2, 0, 1).unsqueeze(0) / 255.0
            state = torch.from_numpy(obs["agent_pos"]).float().unsqueeze(0)
            
            batch = {
                "observation.image": img.cuda(),
                "observation.state": state.cuda(),
            }
            
            # æ¨ç†
            with torch.no_grad():
                action = policy.select_action(batch)
            
            action = action.cpu().numpy().flatten()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            max_reward = max(max_reward, reward)
            step += 1
            
            # è¿›åº¦æ˜¾ç¤ºï¼ˆå¤§æ¨¡å‹æ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if verbose and step % 50 == 0:
                log(f"  [Episode {ep+1}] Step {step}...")
        
        ep_time = time.time() - ep_start
        episode_times.append(ep_time)
        
        success = max_reward >= 1.0
        successes.append(success)
        sum_rewards.append(episode_reward)
        max_rewards.append(max_reward)
        
        # é¢„ä¼°å‰©ä½™æ—¶é—´
        avg_time = np.mean(episode_times)
        remaining = avg_time * (n_episodes - ep - 1)
        
        log(f"[{time.strftime('%H:%M:%S')}] Episode {ep+1}/{n_episodes}: "
            f"{'âœ…' if success else 'âŒ'} reward={episode_reward:.1f}, "
            f"max={max_reward:.3f} | {ep_time:.1f}s | å‰©ä½™: {remaining:.0f}s")
    
    env.close()
    total_time = time.time() - total_start
    
    # æ±‡æ€»ç»“æœ
    results = {
        "pc_success": 100 * np.mean(successes),
        "avg_sum_reward": float(np.mean(sum_rewards)),
        "avg_max_reward": float(np.mean(max_rewards)),
        "n_episodes": n_episodes,
        "total_time_s": total_time,
        "avg_episode_time_s": float(np.mean(episode_times)),
    }
    
    log("\n" + "=" * 60)
    log(f"[{time.strftime('%H:%M:%S')}] ğŸ“Š è¯„ä¼°ç»“æœ:")
    log(f"   pc_success: {results['pc_success']:.1f}%")
    log(f"   avg_sum_reward: {results['avg_sum_reward']:.2f}")
    log(f"   avg_max_reward: {results['avg_max_reward']:.4f}")
    log(f"   æ€»è€—æ—¶: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
    log(f"   å¹³å‡æ¯ episode: {results['avg_episode_time_s']:.1f}s")
    log("=" * 60)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--policy_type", type=str, default="diffusion", 
                        choices=["diffusion", "act"], help="ç­–ç•¥ç±»å‹")
    parser.add_argument("--n_episodes", type=int, default=50, help="è¯„ä¼° episode æ•°")
    parser.add_argument("--output", type=str, default=None, help="ç»“æœä¿å­˜è·¯å¾„ (JSON)")
    parser.add_argument("--quiet", action="store_true", help="å‡å°‘è¾“å‡º")
    
    args = parser.parse_args()
    
    log("=" * 60)
    log(f"ğŸ”¬ æ¨¡å‹è¯„ä¼°")
    log(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    log(f"   ç­–ç•¥ç±»å‹: {args.policy_type}")
    log(f"   è¯„ä¼°æ•°é‡: {args.n_episodes} episodes")
    log("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.exists():
        log(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    if not (model_path / "model.safetensors").exists():
        log(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path / 'model.safetensors'}")
        sys.exit(1)
    
    # åŠ è½½æ¨¡å‹
    policy = load_policy(str(model_path), args.policy_type)
    
    # è¯„ä¼°
    results = evaluate(policy, n_episodes=args.n_episodes, verbose=not args.quiet)
    
    # ä¿å­˜ç»“æœ
    output_path = args.output or (model_path / "eval_results.json")
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    log(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")
    
    return results


if __name__ == "__main__":
    main()
