#!/usr/bin/env python3
"""
é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒ Diffusion Policy å’Œ ACT

ç”¨æ³•:
    python eval_model.py --model_path <æ¨¡å‹è·¯å¾„> [é€‰é¡¹]

ç¤ºä¾‹:
    python eval_model.py --model_path outputs/diffusion_exp/exp1_100k --policy_type diffusion
    python eval_model.py --model_path outputs/act_exp/exp1 --policy_type act --n_episodes 20
"""

import argparse
import sys
import time
import json
from pathlib import Path

import torch
import numpy as np
import gymnasium as gym

# ç¡®ä¿è¾“å‡ºå®æ—¶åˆ·æ–°
def log(msg):
    print(msg, flush=True)


def load_policy(model_path: str, policy_type: str):
    """åŠ è½½ç­–ç•¥æ¨¡å‹å’Œåå¤„ç†å™¨"""
    log(f"[{time.strftime('%H:%M:%S')}] ğŸ“¦ åŠ è½½ {policy_type} æ¨¡å‹: {model_path}")
    
    if policy_type == "diffusion":
        from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy
        policy = DiffusionPolicy.from_pretrained(model_path)
    elif policy_type == "act":
        from lerobot.policies.act.modeling_act import ACTPolicy
        policy = ACTPolicy.from_pretrained(model_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {policy_type}")
    
    policy.eval()
    policy.cuda()
    
    # åŠ è½½é¢„å¤„ç†å™¨ï¼ˆç”¨äºå½’ä¸€åŒ–è¾“å…¥ï¼‰
    preprocessor = None
    preprocessor_path = Path(model_path) / "policy_preprocessor.json"
    if preprocessor_path.exists():
        try:
            from lerobot.processor import PolicyProcessorPipeline
            preprocessor = PolicyProcessorPipeline.from_pretrained(
                pretrained_model_name_or_path=model_path,
                config_filename="policy_preprocessor.json"
            )
            log(f"[{time.strftime('%H:%M:%S')}] âœ… é¢„å¤„ç†å™¨åŠ è½½æˆåŠŸ (ç”¨äºè¾“å…¥å½’ä¸€åŒ–)")
        except Exception as e:
            log(f"[{time.strftime('%H:%M:%S')}] âš ï¸ é¢„å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
    
    # åŠ è½½åå¤„ç†å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–åŠ¨ä½œï¼‰
    postprocessor = None
    postprocessor_path = Path(model_path) / "policy_postprocessor.json"
    if postprocessor_path.exists():
        try:
            from lerobot.processor import PolicyProcessorPipeline
            postprocessor = PolicyProcessorPipeline.from_pretrained(
                pretrained_model_name_or_path=model_path,
                config_filename="policy_postprocessor.json"
            )
            log(f"[{time.strftime('%H:%M:%S')}] âœ… åå¤„ç†å™¨åŠ è½½æˆåŠŸ (ç”¨äºåŠ¨ä½œåå½’ä¸€åŒ–)")
        except Exception as e:
            log(f"[{time.strftime('%H:%M:%S')}] âš ï¸ åå¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
    
    params = sum(p.numel() for p in policy.parameters())
    log(f"[{time.strftime('%H:%M:%S')}] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    log(f"   å‚æ•°é‡: {params:,} ({params/1e9:.2f}B)")
    
    # æ‰“å°å…³é”®é…ç½®
    if hasattr(policy.config, 'horizon'):
        log(f"   horizon: {policy.config.horizon}")
    if hasattr(policy.config, 'n_action_steps'):
        log(f"   n_action_steps: {policy.config.n_action_steps}")
    if hasattr(policy.config, 'down_dims'):
        log(f"   down_dims: {policy.config.down_dims}")
    if hasattr(policy.config, 'dim_model'):
        log(f"   dim_model: {policy.config.dim_model}")
    if hasattr(policy.config, 'n_decoder_layers'):
        log(f"   n_decoder_layers: {policy.config.n_decoder_layers}")
    
    return policy, preprocessor, postprocessor


def evaluate(policy, n_episodes: int = 50, verbose: bool = True, video_dir: str = None, n_video_episodes: int = 3, preprocessor=None, postprocessor=None):
    """è¯„ä¼°æ¨¡å‹
    
    Args:
        policy: ç­–ç•¥æ¨¡å‹
        n_episodes: è¯„ä¼°è½®æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        preprocessor: è¾“å…¥é¢„å¤„ç†å™¨ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        postprocessor: åŠ¨ä½œåå¤„ç†å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
        video_dir: è§†é¢‘ä¿å­˜ç›®å½•ï¼ŒNone åˆ™ä¸å½•åˆ¶
        n_video_episodes: å½•åˆ¶è§†é¢‘çš„ episode æ•°é‡
    """
    import gym_pusht  # ç¡®ä¿ç¯å¢ƒå·²æ³¨å†Œ
    from gymnasium.wrappers import RecordVideo
    
    log(f"\n[{time.strftime('%H:%M:%S')}] ğŸ® åˆ›å»º PushT ç¯å¢ƒ...")
    env = gym.make("gym_pusht/PushT-v0", obs_type="pixels_agent_pos", render_mode="rgb_array")
    
    # æ·»åŠ è§†é¢‘å½•åˆ¶
    if video_dir:
        Path(video_dir).mkdir(parents=True, exist_ok=True)
        env = RecordVideo(
            env, 
            video_folder=video_dir,
            episode_trigger=lambda ep: ep < n_video_episodes,  # åªå½•åˆ¶å‰å‡ ä¸ª episode
            name_prefix="eval"
        )
        log(f"[{time.strftime('%H:%M:%S')}] ğŸ¬ è§†é¢‘å°†ä¿å­˜åˆ°: {video_dir} (å‰{n_video_episodes}ä¸ªepisode)")
    
    successes = []
    avg_rewards = []  # æ¯ä¸ª episode çš„å¹³å‡å¥–åŠ±ï¼ˆreward_sum / stepsï¼‰
    max_rewards = []
    episode_times = []
    
    log(f"[{time.strftime('%H:%M:%S')}] ğŸš€ å¼€å§‹è¯„ä¼° ({n_episodes} episodes)...")
    log("=" * 60)
    
    total_start = time.time()
    
    for ep in range(n_episodes):
        ep_start = time.time()
        obs, info = env.reset()
        policy.reset()
        done = False
        episode_reward = 0
        max_reward = 0
        step = 0
        
        while not done:
            # å‡†å¤‡è¾“å…¥
            img = torch.from_numpy(obs["pixels"]).float().permute(2, 0, 1) / 255.0
            state = torch.from_numpy(obs["agent_pos"]).float()
            
            batch = {
                "observation.image": img,
                "observation.state": state,
            }
            
            # åº”ç”¨é¢„å¤„ç†å™¨ï¼ˆå½’ä¸€åŒ–è¾“å…¥ï¼‰
            if preprocessor is not None:
                batch = preprocessor(batch)
            else:
                # å¦‚æœæ²¡æœ‰é¢„å¤„ç†å™¨ï¼Œæ‰‹åŠ¨æ·»åŠ  batch ç»´åº¦å’Œç§»åˆ° GPU
                batch = {
                    "observation.image": img.unsqueeze(0).cuda(),
                    "observation.state": state.unsqueeze(0).cuda(),
                }
            
            # æ¨ç†
            with torch.no_grad():
                action = policy.select_action(batch)
            
            # åº”ç”¨åå¤„ç†å™¨ï¼ˆåå½’ä¸€åŒ–åŠ¨ä½œï¼‰
            if postprocessor is not None:
                action_dict = {"action": action}
                action_dict = postprocessor(action_dict)
                action = action_dict["action"]
            
            action = action.cpu().numpy().flatten()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            max_reward = max(max_reward, reward)
            step += 1
            
            # è¿›åº¦æ˜¾ç¤ºï¼ˆå¤§æ¨¡å‹æ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if verbose and step % 50 == 0:
                log(f"  [Episode {ep+1}] Step {step}...")
        
        ep_time = time.time() - ep_start
        episode_times.append(ep_time)
        
        # ä½¿ç”¨ç¯å¢ƒè¿”å›çš„ is_successï¼ˆå®˜æ–¹æ ‡å‡†ï¼šcoverage > 0.95ï¼‰
        success = info.get("is_success", False)
        successes.append(success)
        avg_rewards.append(episode_reward / step)  # è¯¥ episode çš„å¹³å‡å¥–åŠ±
        max_rewards.append(max_reward)
        
        # é¢„ä¼°å‰©ä½™æ—¶é—´
        avg_time = np.mean(episode_times)
        remaining = avg_time * (n_episodes - ep - 1)
        
        log(f"[{time.strftime('%H:%M:%S')}] Episode {ep+1}/{n_episodes}: "
            f"{'âœ…' if success else 'âŒ'} reward={episode_reward:.1f}, "
            f"max={max_reward:.3f} | {ep_time:.1f}s | å‰©ä½™: {remaining:.0f}s")
    
    env.close()
    total_time = time.time() - total_start
    
    # æ±‡æ€»ç»“æœ
    results = {
        "pc_success": 100 * np.mean(successes),
        "avg_reward": float(np.mean(avg_rewards)),  # æ‰€æœ‰ episode å¹³å‡å¥–åŠ±çš„å¹³å‡å€¼
        "avg_max_reward": float(np.mean(max_rewards)),
        "n_episodes": n_episodes,
        "total_time_s": total_time,
        "avg_episode_time_s": float(np.mean(episode_times)),
    }
    
    log("\n" + "=" * 60)
    log(f"[{time.strftime('%H:%M:%S')}] ğŸ“Š è¯„ä¼°ç»“æœ:")
    log(f"   pc_success: {results['pc_success']:.1f}%")
    log(f"   avg_reward: {results['avg_reward']:.4f}")  # å¹³å‡å¥–åŠ±ï¼ŒèŒƒå›´ 0-1
    log(f"   avg_max_reward: {results['avg_max_reward']:.4f}")
    log(f"   æ€»è€—æ—¶: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
    log(f"   å¹³å‡æ¯ episode: {results['avg_episode_time_s']:.1f}s")
    log("=" * 60)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--policy_type", type=str, default="diffusion", 
                        choices=["diffusion", "act"], help="ç­–ç•¥ç±»å‹")
    parser.add_argument("--n_episodes", type=int, default=50, help="è¯„ä¼° episode æ•°")
    parser.add_argument("--output", type=str, default=None, help="ç»“æœä¿å­˜è·¯å¾„ (JSON)")
    parser.add_argument("--quiet", action="store_true", help="å‡å°‘è¾“å‡º")
    parser.add_argument("--video_dir", type=str, default=None, help="è§†é¢‘ä¿å­˜ç›®å½• (é»˜è®¤: æ¨¡å‹ç›®å½•/videos)")
    parser.add_argument("--n_video_episodes", type=int, default=3, help="å½•åˆ¶è§†é¢‘çš„ episode æ•°")
    
    args = parser.parse_args()
    
    log("=" * 60)
    log(f"ğŸ”¬ æ¨¡å‹è¯„ä¼°")
    log(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    log(f"   ç­–ç•¥ç±»å‹: {args.policy_type}")
    log(f"   è¯„ä¼°æ•°é‡: {args.n_episodes} episodes")
    log("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.exists():
        log(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    if not (model_path / "model.safetensors").exists():
        log(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path / 'model.safetensors'}")
        sys.exit(1)
    
    # åŠ è½½æ¨¡å‹ã€é¢„å¤„ç†å™¨å’Œåå¤„ç†å™¨
    policy, preprocessor, postprocessor = load_policy(str(model_path), args.policy_type)
    
    # ç¡®å®šè§†é¢‘ç›®å½•ï¼ˆé»˜è®¤ä¿å­˜åˆ°æ¨¡å‹ç›®å½•ä¸‹çš„ videos/ï¼‰
    video_dir = args.video_dir
    if video_dir is None:
        video_dir = str(model_path / "videos")
    
    # è¯„ä¼°
    results = evaluate(
        policy, 
        n_episodes=args.n_episodes, 
        verbose=not args.quiet,
        video_dir=video_dir,
        n_video_episodes=args.n_video_episodes,
        preprocessor=preprocessor,
        postprocessor=postprocessor
    )
    
    # ä¿å­˜ç»“æœ
    output_path = args.output or (model_path / "eval_results.json")
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    log(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")
    
    return results


if __name__ == "__main__":
    main()
